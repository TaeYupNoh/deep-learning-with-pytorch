{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c30f0a",
   "metadata": {},
   "source": [
    "# 8. 컨볼루션을 활용한 일반화\n",
    "## 8.1 컨볼루션\n",
    "### 8.1.1 컨볼루션의 역할\n",
    "- 평행이동 불변성 : 지역화된 패턴이 이미지의 어떤 위치에 있더라도 동일하게 출력에 영향을 주는 성질\n",
    "- 컨볼루션(이산 컨볼루션, discrete convolution) : 2차원 이미지에 가중치 행렬을 스칼라곱을 수행하는 것, 여기서 가중치 행렬은 커널(kernel)이라 함  \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/107118671/189571492-0536a7d4-7b1a-45be-a385-4c999059f78b.png\" width=\"70%\" height=\"70%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b5537",
   "metadata": {},
   "source": [
    "## 8.2 컨볼루션 사용해보기\n",
    "- nn.Conv1d는 시계열용 nn.Conv2d는 이미지용 nn.Con3d는 용적 데이터나 동영상용\n",
    "- CIFAR-10 데이터에 대해서는 nn.Conv2d 사용, 전달 인자는 최소 입력 피처수, 출력 피처 수, 커널의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f00c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2685244b250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d38df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f842454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../data-unversioned/p1ch6/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "100bc836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a7a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d4b622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43dd02f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1556f021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae243b63",
   "metadata": {},
   "source": [
    "### 8.2.1 경계 패딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce1179fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f624b",
   "metadata": {},
   "source": [
    "### 8.2.2 컨볼루션으로 피처 찾아내기\n",
    "- 가중치를 직접 설정해서 컨볼루션에서 어떤 일이 일어나는 지 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f236986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e42ca9d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6befcc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()\n",
    "\n",
    "# 커널을 직접 바꾸기\n",
    "# 가로로 인접한 두 영역 사이의 수직 경계를 탐색하는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6f3dd",
   "metadata": {},
   "source": [
    "### 8.2.3 깊이와 풀링으로 한 단계 더 인식하기\n",
    "- 이미지 내 그림이 꽤 크다면 어떻게 해야 할까?\n",
    "- 큰 이미지에서 작은 이미지로 : 다운샘플링 (네 개의 픽셀 평균하기 / 네 개의 픽셀 중 최댓값 : 맥스 풀링 / 스트라이드하며 컨볼루션 수행하되 n번째 픽셀만 계산하기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d34c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img.unsqueeze(0))\n",
    "\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327a1f5",
   "metadata": {},
   "source": [
    "### 8.2.4 우리의 신경망에 적용하기\n",
    "- 더 나은 성능을 위해 컨볼루션과 다운샘플링 결합하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0efa3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # ...\n",
    "            )\n",
    "\n",
    "# 첫 번째 컨볼루션은 3 RGB 채널을 16개의 독립적인 피처를 만들어 새와 비행기에 대한 저수준의 피처 찾아냄\n",
    "# Tanh 활성함수 적용\n",
    "# 결과로 만들어진 16채널의 32 * 32 이미지는 MaxPool2d를 통해 16채널 16 * 16 이미지로 다운샘플링\n",
    "# 8채널 16 * 16 출력 만드는 다른 컨볼루션으로 들어감\n",
    "# Tanh 활성함수 적용\n",
    "# 8채널 8 * 8 폴링 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bc62dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # ... 중요한게 하나 빠져있음\n",
    "            nn.Linear(8 * 8 * 8, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56f51f",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/107118671/189581278-be114606-9dfe-49e5-b143-5fecb20f9041.png\" width=\"70%\" height=\"70%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb5361cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list\n",
    "# 총 파라미터 수 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c67e2dc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)"
     ]
    }
   ],
   "source": [
    "model(img.unsqueeze(0))\n",
    "# 8채널의 8 * 8 이미지를 512요소를 가진 1차원 벡터로 차원 정보를 변경해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c0904",
   "metadata": {},
   "source": [
    "## 8.3 nn.Module 서브클래싱하기\n",
    "- nn.Module의 서브클래스를 직접 만드는 법을 배워서 nn.Sequential처럼 사용할 수 있게 할 예정\n",
    "### 8.3.1 nn.Module로 정의된 우리의 신경망\n",
    "- 우리의 신경망을 서브모듈로 작성해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26e9c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8) # 앞에서 놓쳤던 차원 정보 변경\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c2effd",
   "metadata": {},
   "source": [
    "### 8.3.2 파이토치가 파라미터와 서브모듈을 유지하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13046ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01f89e",
   "metadata": {},
   "source": [
    "##### 함수형 API는 파라미터가 없는 nn.Tanh나 nn.MaxPool2d 같은 서브 모듈은 굳이 등록할 필요가 없고 view 호출 처럼 forward함수에서 직접 호출하는게 쉽지 않을까라는 아이디어에서 비롯됨\n",
    "### 8.3.3 함수형 API\n",
    "- 파이토치는 모든 nn 모듈에 대해 함수형(funtional)을 제공\n",
    "- 함수형이란 내부 상태가 없다는 의미로써 출력값이 전적으로 입력 인자에 의존하는 것을 말함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49656e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa447f45",
   "metadata": {},
   "source": [
    "- 8.3.1절에서 정의한 Net과 동일하면서 훨씬 간결한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "312ac8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0234, 0.0825]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8336a2",
   "metadata": {},
   "source": [
    "## 8.4 우리가 만든 컨볼루션 신경망 훈련시키기\n",
    "- 바깥 루프는 에포크 단위로 돌며, 안쪽 루프는 Dataset에서 배치를 만드는 DataLoader 단위로 돎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0fdf77",
   "metadata": {},
   "source": [
    "#### 각 루프에서,  \n",
    "#### 1. 모델에 입력값 넣기(순방향 전달)\n",
    "#### 2. 손실값 계산(순방향 전달)\n",
    "#### 3. 이전 기울기값 0으로 리셋\n",
    "#### 4. loss.backward() 호출해 모든 파라미터에 대한 손실값의 기울기 계산(역방향 전달)\n",
    "#### 5. 옵티마이저를 통해 손실값 낮추도록 파라미터 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec9af9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime  # 파이썬에 포함된 datetime 모듈 사용\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:  \n",
    "        # 데이터 로더가 만들어준 배치 안에서 데이터셋을 순회함\n",
    "            \n",
    "            outputs = model(imgs)  \n",
    "            # 모델에 배치 넣어줌\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)  \n",
    "            # 최소화하려는 손실값 계산\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            # 마지막에 이전 기울기 값 지움\n",
    "            \n",
    "            loss.backward()  \n",
    "            # 역전파 수행, 신경망이 학습할 모든 파라미터에 대한 기울기를 계산\n",
    "            \n",
    "            optimizer.step()  \n",
    "            # 모델 업데이트\n",
    "\n",
    "            loss_train += loss.item()  \n",
    "            # 에포크동안 확인한 손실값 모두 더함. 기울기값을 꺼내고자 .item() 사용해 손실값으로 파이썬 수로 변환 시킴\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))  \n",
    "                # 배치 단위의 평균 손실값 구하기 위해 훈련 데이터 로더의 길이로 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "556bb01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-12 19:48:40.432092 Epoch 1, Training loss 0.5892274309495452\n",
      "2022-09-12 19:49:11.069956 Epoch 10, Training loss 0.330605825516069\n",
      "2022-09-12 19:49:45.501019 Epoch 20, Training loss 0.3004183064980112\n",
      "2022-09-12 19:50:20.790662 Epoch 30, Training loss 0.27614913434739324\n",
      "2022-09-12 19:50:56.310515 Epoch 40, Training loss 0.25298165117099786\n",
      "2022-09-12 19:51:31.184836 Epoch 50, Training loss 0.23757346169014645\n",
      "2022-09-12 19:52:05.464619 Epoch 60, Training loss 0.22288798683198394\n",
      "2022-09-12 19:52:41.686796 Epoch 70, Training loss 0.20795749208539915\n",
      "2022-09-12 19:53:18.540276 Epoch 80, Training loss 0.1929596746992913\n",
      "2022-09-12 19:53:57.022636 Epoch 90, Training loss 0.1782363099372311\n",
      "2022-09-12 19:54:34.105995 Epoch 100, Training loss 0.16900125345227066\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)  # <1>\n",
    "\n",
    "model = Net()  #  <2>\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
    "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
    "\n",
    "training_loop(  # <5>\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35ee3e",
   "metadata": {},
   "source": [
    "### 8.4.1 정확도 측정\n",
    "- 훈련셋과 검증셋을 통한 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fc4db77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.90\n",
      "Accuracy val: 0.87\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  \n",
    "        # 파라미터 업데이트하지 않을 것이므로 기울기는 필요 없음\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) \n",
    "                # 가장 높은 값을 가진 인덱스 출력\n",
    "                total += labels.shape[0]  \n",
    "                # 예제 수를 세어서 total을 배치 크기만큼 증가시킴\n",
    "                correct += int((predicted == labels).sum())  # \n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b19e75",
   "metadata": {},
   "source": [
    "### 8.4.2 모델을 저장하고 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74db3e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39e732d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net()  \n",
    "# 모델 상태를 저장하고 나중에 읽는 사이에 Net의 정의가 바뀌지 않았는지 확인 필요\n",
    "loaded_model.load_state_dict(torch.load(data_path\n",
    "                                        + 'birds_vs_airplanes.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d8ff4",
   "metadata": {},
   "source": [
    "### 8.4.3 GPU에서 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05f61e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "151eb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)  # imgs와 labels를 옮기는 이 두 줄만 이전과 다름\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f31141c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-12 19:55:39.186440 Epoch 1, Training loss 0.5838478852988808\n",
      "2022-09-12 19:56:11.880021 Epoch 10, Training loss 0.3333204200692997\n",
      "2022-09-12 19:56:48.400222 Epoch 20, Training loss 0.2976608706317889\n",
      "2022-09-12 19:57:27.018765 Epoch 30, Training loss 0.2723341862297362\n",
      "2022-09-12 19:58:05.819478 Epoch 40, Training loss 0.2511897647077111\n",
      "2022-09-12 19:58:41.702436 Epoch 50, Training loss 0.23055718383591645\n",
      "2022-09-12 19:59:16.161456 Epoch 60, Training loss 0.2158295685888096\n",
      "2022-09-12 19:59:49.862987 Epoch 70, Training loss 0.19999624603683022\n",
      "2022-09-12 20:00:30.954851 Epoch 80, Training loss 0.18691539137986055\n",
      "2022-09-12 20:01:11.562918 Epoch 90, Training loss 0.17178598945592619\n",
      "2022-09-12 20:01:51.399910 Epoch 100, Training loss 0.15749303377263105\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = Net().to(device=device)  # 모델도 GPU(설정한 장치)로 옮김\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "556b87c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.98\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "all_acc_dict = collections.OrderedDict()\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    accdict = {}\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "        accdict[name] = correct / total\n",
    "    return accdict\n",
    "\n",
    "all_acc_dict[\"baseline\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f95fabd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(torch.load(data_path\n",
    "                                        + 'birds_vs_airplanes.pt',\n",
    "                                        map_location=device))\n",
    "# 가중치를 로딩할 때 파이토치가 기억하는 디바이스 정보 덮어쓰기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3abe5a",
   "metadata": {},
   "source": [
    "## 8.5 모델 설계\n",
    "- 좀 더 복잡한 문제 해결을 위한 설계\n",
    "- 이 절에서는 개념적인 도구를 알아보며 최신 논문이 제공하는 파이토치 구현을 이해하고 할 수 있도록 하는 것\n",
    "### 8.5.1 메모리 용량 늘리기: 너비\n",
    "- 첫 번째 컨볼루션의 출력 채널 수를 더 크기 하고 이어지는 계층도 여기에 맞춰 키우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc1b70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의에서 하드코딩을 피하기위해 파라미터를 init에 전달하고 너비를 파라미터화해서 forward함수에서 view를 호출할 때도 이를 고려\n",
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8365fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-12 17:00:50.526177 Epoch 1, Training loss 0.5413629414549299\n",
      "2022-09-12 17:01:49.965240 Epoch 10, Training loss 0.31306587586736984\n",
      "2022-09-12 17:03:05.379659 Epoch 20, Training loss 0.27742175700937866\n",
      "2022-09-12 17:04:28.763012 Epoch 30, Training loss 0.24679041165075485\n",
      "2022-09-12 17:05:50.133250 Epoch 40, Training loss 0.21807068757190826\n",
      "2022-09-12 17:07:07.501334 Epoch 50, Training loss 0.19153281714126563\n",
      "2022-09-12 17:08:24.752262 Epoch 60, Training loss 0.1676977397814678\n",
      "2022-09-12 17:09:38.605677 Epoch 70, Training loss 0.1454492052601781\n",
      "2022-09-12 17:10:57.616240 Epoch 80, Training loss 0.12472249961392895\n",
      "2022-09-12 17:12:12.776145 Epoch 90, Training loss 0.10542512376596973\n",
      "2022-09-12 17:13:31.259013 Epoch 100, Training loss 0.08798113237520691\n",
      "Accuracy train: 0.97\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "model = NetWidth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "all_acc_dict[\"width\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a176f8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38386"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992f472",
   "metadata": {},
   "source": [
    "### 8.5.2 모델이 수렴하고 일반화하도록 돕는 방법: 정규화\n",
    "- 모델 훈련에 중요한 두 단계 1) 최적화 단계 : 훈련셋에 대한 손실값을 줄이는 경우\n",
    "- 모델 훈련에 중요한 두 단계 2) 일반화 단계 : 이전에 겪어보지 않은 검증셋에서도 동작하게 하는 것\n",
    "- 이 두 단계를 위한 수학적 도구를 모아서 '정규화(regularization)'라는 이름 붙이기도 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b8369",
   "metadata": {},
   "source": [
    "### 파라미터 제어하기: 가중치 페널티\n",
    "#### 일반화를 안정적으로 수행하기 위한 첫 번째 방법으로 손실값에 정규화 항을 넣는 것이 있음\n",
    "#### 이 정규화 항을 조작해서 모델의 가중치가 상대적으로 작게 만듦\n",
    "#### 큰 가중치 값에 페널티를 부과하는 셈이 되고, 개별 샘플에 맞춰서 얻는 이득이 상대적으로 줄어들게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26a6f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치의 정규하는 손실값에 항을 하나 추가하는 것으로 매우 손쉽게 구현 가능\n",
    "\n",
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n",
    "                        train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum()\n",
    "                          for p in model.parameters())  # L1 정규화라면 pow(2.0)을 abs()로 바꿈\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b489f1b",
   "metadata": {},
   "source": [
    "### 입력 하나에 너무 의존하지 않기: 드랍아웃\n",
    "#### 훈련을 반복할 때마다 신경망의 뉴런 출력을 랜덤하게 0으로 만드는 작업을 일어나게 하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38e1a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662ce69",
   "metadata": {},
   "source": [
    "### 활성 함수 억제하기: 배치 정규화\n",
    "#### 입력 범위를 신경망의 활성 함수로 바꿔서 미니 배치가 원하는 분포를 가지게 하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62f2ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, \n",
    "                               padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b7e49",
   "metadata": {},
   "source": [
    "### 8.5.3 더 복잡한 구조를 배우기 위해 깊이 파헤치기: 깊이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9cf3c",
   "metadata": {},
   "source": [
    "### 스킵 커넥션\n",
    "- 딥러닝 모델의 계층은 20개 이상 늘어나기 어려웠음\n",
    "- 곱셈이 체인이 길게 이어지는 경우 기울기값에 기여하는 파라미터가 사라려버려(vanishing) 파라미터 같은 값들이 적절하게 업데이트 되지 않음\n",
    "- 잔차 신경망(residual network)인 레즈넷(ResNet)에서 스킵 커넥션(skip connection)을 사용\n",
    "- 스킵 커넥션은 입력을 계층 블럭의 출력에 연결하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "967a9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4074c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a1ff81",
   "metadata": {},
   "source": [
    "### 파이토치로 매우 깊은 모델 만들기\n",
    "- 컨볼루션과 활성 함수, 스킵 커넥션으로 이뤄진 블럭을 위한 연산을 제공하는 모듈 서브클래스 정의\n",
    "- 블럭에 배치 정규화를 넣어 훈련 도중에 기울기 값이 없어지는 것을 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76d9d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
    "                              padding=1, bias=False)  # BathNorm 계층은 편향값의 효과 상쇄하므로, 관례상 이를 생략\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                      nonlinearity='relu')  # 커스텀 초기화(.kaiming_normal_은 표준편차를 가지는 표준 랜덤 요소로 초기화해줌)\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92375601",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89c0b1",
   "metadata": {},
   "source": [
    "### 초기화\n",
    "- 파이토치의 기본 가중치 초기화는 완벽하지 않음\n",
    "- 우리 모델은 수렴하지 않았고 일반적인 표기화 방법(가중치 변화량 감소와 배치 정규화를 위한 평균값 0과 단위 분산 출력 등)도 살펴봄\n",
    "- 신경망이 수렴하지 않는 경우 배치 정규화에서 출력 분산을 절반으로 줄이기도 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c848f",
   "metadata": {},
   "source": [
    "### 8.5.4 모델의 성능 비교\n",
    "<img src=\"https://user-images.githubusercontent.com/107118671/189605169-6f479feb-3533-435c-a5bd-92c59e98d30d.png\" width = \"70%\" height=\"70%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31dbd0b",
   "metadata": {},
   "source": [
    "## 8.6 결론\n",
    "- 많은 작업을 거치면서 이미지를 필터링하는 모델을 만듦\n",
    "- 이제는 폐암을 자동으로 진단하는 문제를 다룰 예정"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
