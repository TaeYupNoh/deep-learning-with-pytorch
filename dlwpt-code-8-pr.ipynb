{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9707f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1aee4d0e250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d4b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567965db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../data-unversioned/p1ch6/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98bf48a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc8fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8052287e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size=5)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48485520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 5, 5]), torch.Size([16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3184e99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 28, 28]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f4d54e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=5, padding=2)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49625b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1adc20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "460cf3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()\n",
    "\n",
    "# 커널을 직접 바꾸기\n",
    "# 가로로 인접한 두 영역 사이의 수직 경계를 탐색하는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16f4eb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img.unsqueeze(0))\n",
    "\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7e0385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, padding=2),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 8, kernel_size=5, padding=2),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # ... 중요한게 하나 빠져있음\n",
    "            nn.Linear(8 * 8 * 8, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ec9fad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20906, [1200, 16, 3200, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list\n",
    "# 총 파라미터 수 세기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac75faf",
   "metadata": {},
   "source": [
    "- 파라미터 수 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9646158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=5, padding=2)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8) # 앞에서 놓쳤던 차원 정보 변경\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a410ce1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20906, [1200, 16, 3200, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b8acf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수형 API 이용한 Net 간결 버전 코드\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "764fd758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1232, -0.1478]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb80f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime  # 파이썬에 포함된 datetime 모듈 사용\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:  # 데이터 로더가 만들어준 배치 안에서 데이터셋을 순회함\n",
    "            \n",
    "            outputs = model(imgs)  # 모델에 배치 넣어줌\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)  # 최소화하려는 손실값 계산\n",
    "\n",
    "            optimizer.zero_grad()  # 마지막에 이전 기울기 값 지움\n",
    "            \n",
    "            loss.backward()  # 역전파 수행, 신경망이 학습할 모든 파라미터에 대한 기울기를 계산\n",
    "            \n",
    "            optimizer.step()  # 모델 업데이트\n",
    "\n",
    "            loss_train += loss.item()  \n",
    "            # 에포크동안 확인한 손실값 모두 더함. 기울기값을 꺼내고자 .item() 사용해 손실값으로 파이썬 수로 변환 시킴\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))  # 배치 단위의 평균 손실값 구하기 위해 훈련 데이터 로더의 길이로 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "570d5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-12 17:33:44.531487 Epoch 1, Training loss 0.5596596547372782\n",
      "2022-09-12 17:34:21.444900 Epoch 10, Training loss 0.31791146365320605\n",
      "2022-09-12 17:35:04.091014 Epoch 20, Training loss 0.28216634254167033\n",
      "2022-09-12 17:35:52.692469 Epoch 30, Training loss 0.25078650108378403\n",
      "2022-09-12 17:36:40.776571 Epoch 40, Training loss 0.22250529919650144\n",
      "2022-09-12 17:37:30.199122 Epoch 50, Training loss 0.19829885984302326\n",
      "2022-09-12 17:38:20.534719 Epoch 60, Training loss 0.17688316301365567\n",
      "2022-09-12 17:39:14.228640 Epoch 70, Training loss 0.1551471356848243\n",
      "2022-09-12 17:40:03.388869 Epoch 80, Training loss 0.13819130642968377\n",
      "2022-09-12 17:40:50.493775 Epoch 90, Training loss 0.118316128992351\n",
      "2022-09-12 17:41:45.360974 Epoch 100, Training loss 0.10272454501242395\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)  # <1>\n",
    "\n",
    "model = Net()  #  <2>\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
    "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
    "\n",
    "training_loop(  # <5>\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50d6e658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.96\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # 파라미터 업데이트하지 않을 것이므로 기울기는 필요 없음\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # 가장 높은 값을 가진 인덱스 출력\n",
    "                total += labels.shape[0]  # 예제 수를 세어서 total을 배치 크기만큼 증가시킴\n",
    "                correct += int((predicted == labels).sum())  # \n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c34e8",
   "metadata": {},
   "source": [
    "- 더욱 과적합이 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af55deb",
   "metadata": {},
   "source": [
    "- kernel_size = (1,3)\n",
    "\n",
    "https://stackoverflow.com/questions/59946176/non-squared-convolution-kernel-size\n",
    "\n",
    "https://stats.stackexchange.com/questions/351115/convolution-with-a-non-square-kernel\n",
    "\n",
    "https://towardsdatascience.com/types-of-convolution-kernels-simplified-f040cb307c37\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736374e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
